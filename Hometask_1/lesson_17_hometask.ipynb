{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29203fc1",
   "metadata": {},
   "source": [
    "1. Using a Spacy, create a keywords extractor that should do the following things:\n",
    " - Take some text (article like) as an input.\n",
    " - Remove all stop words from the text.\n",
    " - Extract all the Nouns from text and sort them by count and return in descending order with amount of occurrences. \n",
    " - Extract all the Verbs from text and sort them by count and return in descending order with amount of occurrences.  \n",
    " - Extract all the Numbers from text and sort them by count and return in descending order with amount of occurrences. \n",
    " - Extract all the Named Entities from the text, group them into 4 groups (Location, Person, Organization, Misc.) and return groups in descending order with amount of occurrences. \n",
    "\n",
    "\n",
    "2. Using multilingual USE, align strings in English and Russian texts:\n",
    " - Download multilingual USE model - https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
    " - Read \"./data/corpora/en.txt\" and \"./data/corpora/ru.txt\" files\n",
    " - Align English strings with their Russian analogues using mUSE\n",
    " \n",
    " \n",
    "3. Using the USE, create a Duplicate Phrase Finder that will do the following:\n",
    " - Take some large text as an input.\n",
    " - Separates text to SENTENCES (phrases). \n",
    " - Finds semantically similar strings (cosine similarity >=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2df76eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1067b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(input_file):\n",
    "    with open(input_file) as f:\n",
    "        article = f.read()\n",
    "        #Output little part of article\n",
    "        print(f'Article: \\n {article[:500]} \\n')\n",
    "        \n",
    "    return article\n",
    "\n",
    "\n",
    "input_file = 'data/golf_article.txt'\n",
    "doc = nlp(get_document(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb649176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of tokens before and after removing stop words: 1069/634\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(doc):\n",
    "    all_stopwords = nlp.Defaults.stop_words\n",
    "    doc_before_cleaning_lenght = len(doc)\n",
    "    doc_without_stop_words = [token for token in doc if token.text not in all_stopwords]\n",
    "    print('Amount of tokens before and after removing stop words: {}/{}'\\\n",
    "         .format(doc_before_cleaning_lenght, len(doc_without_stop_words)))\n",
    "    return doc\n",
    "\n",
    "\n",
    "doc = remove_stop_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da50a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_pos(doc):\n",
    "    nouns_list = []\n",
    "    numbers_list = []\n",
    "    verbs_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_=='NOUN':\n",
    "            nouns_list.append(token.lemma_)\n",
    "        elif token.pos_=='NUM':\n",
    "            numbers_list.append(token.lemma_)\n",
    "        elif token.pos_=='VERB':\n",
    "            verbs_list.append(token.lemma_)\n",
    "            \n",
    "    print('Most freq nouns tokens:\\n {} \\n'.format(Counter(nouns_list).most_common(10)))\n",
    "    print('Most freq nouns tokens:\\n {} \\n'.format(Counter(numbers_list).most_common(10)))\n",
    "    print('Most freq nouns tokens:\\n {} \\n'.format(Counter(verbs_list).most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b5aa60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most freq nouns tokens:\n",
      " [('golf', 28), ('ball', 14), ('hole', 14), ('player', 13), ('stroke', 11), ('golfer', 11), ('club', 10), ('par', 7), ('course', 6), ('number', 5)] \n",
      "\n",
      "Most freq nouns tokens:\n",
      " [('one', 6), ('two', 4), ('18', 3), ('500', 2), ('hundred', 1), ('1.62', 1), ('46', 1), ('14', 1), ('1', 1), ('9', 1)] \n",
      "\n",
      "Most freq nouns tokens:\n",
      " [('have', 13), ('play', 12), ('hit', 7), ('use', 7), ('make', 6), ('be', 5), ('call', 5), ('start', 5), ('finish', 3), ('take', 3)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_pos(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "79b730eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG: \n",
      " [('Golf\\n \\nGolf', 1), ('Golf Equipment', 1), ('Par', 1), ('Eagle', 1), ('Royal  St. Andrews Golf Club', 1), ('Royal St. Andrews Golf Club - Gordon McKinlay\\n \\n', 1), ('PGA', 1), ('Severiano Ballesteros', 1)]\n",
      "\n",
      "PERSON: \n",
      " [('Birdie', 1), ('Saint Andrews', 1), ('Bernhard Langer', 1), ('Gary Player', 1), ('Tiger Woods', 1)]\n",
      "\n",
      "LOCATION: \n",
      " [('Scotland', 1), ('Britain', 1), ('Calcutta', 1), ('India', 1), ('USA', 1), ('Canada', 1), ('Spain', 1), ('Germany', 1), ('South Africa', 1), ('Europe', 1)]\n",
      "\n",
      "MISC: \n",
      " [('first', 4), ('18', 3), ('two', 3), ('one', 3), ('Today', 2), ('British', 2), ('American', 2), ('Millions', 1), ('thousands', 1), ('millions', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_named_entities(doc):\n",
    "    label_dict = {'ORG':[],\n",
    "                  'PERSON':[],\n",
    "                  'LOCATION':[],\n",
    "                  'MISC':[]}\n",
    "    sampling_of_location = ['LOC', 'GPE']\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in label_dict.keys():\n",
    "            label_dict[ent.label_].append(ent.text)\n",
    "        elif ent.label_ in sampling_of_location:\n",
    "            label_dict['LOCATION'].append(ent.text)\n",
    "        else:\n",
    "            label_dict['MISC'].append(ent.text)\n",
    "            \n",
    "    for group in label_dict.keys():\n",
    "        print(f'{group}: \\n {Counter(label_dict[group]).most_common(10)}\\n')\n",
    "\n",
    "\n",
    "get_named_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc7148fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "ORDINAL\n",
      "ORG\n",
      "QUANTITY\n"
     ]
    }
   ],
   "source": [
    "for i, ent in enumerate(doc.ents):\n",
    "    if i==10:\n",
    "        break\n",
    "    print(ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669543d",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56160702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57afbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3205ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = []\n",
    "ru = []\n",
    "with open(\"./data/corpora/en.txt\") as f:\n",
    "    for line in f.readlines()[:50]:\n",
    "        en.append(line.strip())\n",
    "        \n",
    "with open(\"./data/corpora/ru.txt\") as f:\n",
    "    for line in f.readlines()[:50]:\n",
    "        ru.append(line.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0c9789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['How do you explain this progression?',\n",
       "  \"Cigarettes are linked to 85% of lung cancer cases, this massively damages people's health.\",\n",
       "  'Everything moves very fast in football',\n",
       "  \"You're never going to win 4-0 every weekend - we're not FC Barcelona!\",\n",
       "  'We got out of Afghanistan.',\n",
       "  'French troops have left their area of responsibility in Afghanistan'],\n",
       " ['Курение связано с 85% случаев рака легких. Оно наносит колоссальный вред здоровью людей.',\n",
       "  'В футболе все происходит очень быстро.',\n",
       "  'Французские войска покинули свою зону ответственности в Афганистане',\n",
       "  'Мы никогда не сможем выигрывать каждые выходные со счетом 4-0.',\n",
       "  'Мы ушли из Афганистана.',\n",
       "  'Как вы объясните этот рост?'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en, ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6eec04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_of_sentence(sent1, sent2):\n",
    "    message = [sent1, sent2]\n",
    "    en_emb, ru_emb = embed(message)\n",
    "    similarity_of_sentences = np.inner(en_emb, ru_emb)\n",
    "    \n",
    "    return similarity_of_sentences\n",
    "\n",
    "\n",
    "for i, en_sentence in enumerate(en):\n",
    "    for j, ru_sentence in enumerate(ru):\n",
    "        similarity = get_similarity_of_sentence(en_sentence, ru_sentence)\n",
    "        if (similarity>.6) and (i!=j):\n",
    "            ru[i], ru[j] = ru[j], ru[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5512fc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['How do you explain this progression?',\n",
       "  \"Cigarettes are linked to 85% of lung cancer cases, this massively damages people's health.\",\n",
       "  'Everything moves very fast in football',\n",
       "  \"You're never going to win 4-0 every weekend - we're not FC Barcelona!\",\n",
       "  'We got out of Afghanistan.',\n",
       "  'French troops have left their area of responsibility in Afghanistan'],\n",
       " ['Как вы объясните этот рост?',\n",
       "  'Курение связано с 85% случаев рака легких. Оно наносит колоссальный вред здоровью людей.',\n",
       "  'В футболе все происходит очень быстро.',\n",
       "  'Мы никогда не сможем выигрывать каждые выходные со счетом 4-0.',\n",
       "  'Мы ушли из Афганистана.',\n",
       "  'Французские войска покинули свою зону ответственности в Афганистане'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en, ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b37a71",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848f7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bb2cfa6-a563-4485-93c9-6954ac45f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: \n",
      " So much of the family history of this artist as it is needful to repeat,\n",
      "or the reader will care to learn, may be briefly told: it begins with\n",
      "his grandfather, who was a jeweller settled in London, where, in\n",
      "1761,[2] his father, John Landseer, was born. The senior was on intimate\n",
      "terms with Peter, father of the lawyer and politician, Sir Samuel\n",
      "Romilly. Peter Romilly was descended from a distinguished French family,\n",
      "the first of whom known in this country settled near London after the\n",
      "revocation \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It would have been better for\\\\nReynolds’s reputation if he had restricted\\nhimself to that mode of art\\\\nin which he was a master.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'data/Sir-Edwin-Landseer-Frederick-G--St.txt'\n",
    "doc = get_document(input_file)\n",
    "doc_sentences = sent_tokenize(doc)\n",
    "doc_sentences[len(doc_sentences)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfdade62-d1cf-4c13-8af5-eef74885fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DuplicatePhraseFinder(doc):\n",
    "    doc_sentences = sent_tokenize(doc)\n",
    "\n",
    "    for i, sent1 in enumerate(doc_sentences):\n",
    "        for j, sent2 in enumerate(doc_sentences):\n",
    "            if len(sent1)>5:\n",
    "                if (get_similarity_of_sentence(sent1, sent2)>.8) and (i!=j):\n",
    "                    print(f'id: {i}, Sentence: ({sent1})\\nSimilar to \\nid:{j} Sentence: ({sent2})\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bfd3d3d-a52a-4380-b962-1a85c333c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 30, Sentence: (It would have been better for\n",
      "Reynolds’s reputation if he had restricted himself to that mode of art\n",
      "in which he was a master.)\n",
      "Similar to \n",
      "id:40 Sentence: (It would have been better for\\nReynolds’s reputation if he had restricted\n",
      "himself to that mode of art\\nin which he was a master.)\n",
      "\n",
      "\n",
      "id: 40, Sentence: (It would have been better for\\nReynolds’s reputation if he had restricted\n",
      "himself to that mode of art\\nin which he was a master.)\n",
      "Similar to \n",
      "id:30 Sentence: (It would have been better for\n",
      "Reynolds’s reputation if he had restricted himself to that mode of art\n",
      "in which he was a master.)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DuplicatePhraseFinder(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
